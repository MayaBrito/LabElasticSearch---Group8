{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratório Elastic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atividade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexto\n",
    "O objetivo deste laboratório é explorar diferentes mecanismos de busca dentro e fora do Elastic Search!\n",
    "Para isto, iremos explorar uma base de verificações de notícia de uma agência de checagem, chamada [Lupa](https://lupa.uol.com.br/).\n",
    "O papel de uma agência de checagem é analisar uma notícia e verificar sua veracidade, gerar um veredito e retornar sua análise.\n",
    "\n",
    "Neste laboratório, iremos nos preocupar apenas com o texto da análise e em como recuperá-los através de diferentes estratégias de busca.\n",
    "\n",
    "## Tarefas\n",
    "### Tarefa 1\n",
    "- Cada uma das equipes receberá 2 queries fixas, chamadas de QF1 e QF2, e a tarefa do grupo será a criação de mais 2 queries, chamadas de QP1 e QP2. As queries devem seguir o formato de uma pergunta ou declaração textual e ela será usada na próxima tarefa para realizar buscas. As equipes podem se inspirar nas notícias fornecidas (presentes no csv da tarefa 2) para montá-las. Cada query deve ser composta por até 100 caracteres.\n",
    "- A equipe deve enviar suas QP1 e QP2 em um formulário que será disponibilizado no classroom e no discord.\n",
    "\n",
    "### Tarefa 2\n",
    "- Após a criação das QP1 e QP2, cada equipe possuirá 4 queries (QF1; QF2; QP1 e QP2).\n",
    "- Cada grupo irá realizar 4 tipos de busca. Uma busca léxica com BM25 (pelo ElasticSearch), uma busca semântica (pelo ElasticSearch), uma busca híbrida (manualmente utilizando as duas buscas anteriores) e uma estratégia de busca de sua preferência (neste lab ela será chamada de busca criativa), diferente das anteriores.\n",
    "- As buscas léxica e semântica já estão implementadas, os grupos devem gerar a implementação das buscas híbridas e da busca criativa.\n",
    "- A busca criativa será a busca usada para a competição!\n",
    "- Cada grupo durante a implementação de suas buscas deve testar diferentes pré-processamentos nos dados para verificar como os resultados podem melhorar. Alguns pré-processamentos já foram implementados, mas os grupos não precisam se limitar aos pré-processamentos apresentados. Vocês podem buscar na internet implementações de outros pré-processamentos, criar padrões regex, etc. Observe que os dados precisarão ser reindexados no ElasticSearch sempre que os pré-processamentos forem modificados (inseridos, removidos ou mudados de ordem)!\n",
    "- Enquanto vocês testam seus pré-processamentos e algoritmos de busca, ao analisar os resultados obtidos para uma determinada query, anotem o nível de relevância de cada documento lido em relação à query. Observe que esse procedimento pode ser realizado continuamente durante o processo de implementação, pois a relevância de um documento para uma query independe de implementação de buscadores. Essa anotação deve ser realizada (online) em uma planilha no google drive, que será fornecida pela organização do Lab. Caso mais de um integrante do grupo anote valores diferentes na planilha, não tem problemas. Nesse caso, iremos considerar a média dos valores.\n",
    "    - A planilha tem o seguinte formato:\n",
    "        - doc_id, query_id, relevance\n",
    "        - 120, QP1, 2\n",
    "        - 487, QP1, 1\n",
    "        - ...\n",
    "    - Cada busca deve possuir pelo menos 10 resultados anotados.\n",
    "    - Não modifique o código que realiza o carregamento dos dados para que o doc_id seja consistente com os demais grupos!\n",
    "    - A rotulação deve possui uma gradação em 3 níveis:\n",
    "        - 0: Não é Relevante\n",
    "        - 1: Pouco Relevante\n",
    "        - 2: Muito Relevante \n",
    "- A entrega desta tarefa será feita através de um formulário para entregar um .zip contendo todo o repositório e o código utilizado autocontido (com listagem das dependências utilizadas no requirements.txt se necessário) e suas buscas implementadas. Este código será reexecutado, então organize o código para que ele possa ser reexecutado em outra máquina.\n",
    "- Quando esgotar o prazo de envio da tarefa a planilha será bloqueada para modificações.\n",
    "- A partir deste momento, nenhum grupo poderá alterar suas buscas novamente, então tenham ciência que esta será sua implementação final que será usada para a competição.\n",
    "\n",
    "### Tarefa 3\n",
    "- O grupo receberá (pelo discord) 3 queries adicionais (QA1; QA2 e QA3).\n",
    "- A tarefa do grupo será a execução do MESMO CÓDIGO submetido na tarefa anterior com estas 3 queries e a anotação dos dados (a planilha de anotações será disponibilizada pelo discord).\n",
    "- Nesta etapa, o grupo NÃO PODE FAZER ALTERAÇÕES nos pré-processamentos definidos ou nas implementações de suas buscas.\n",
    "- Quando esgotar o prazo de envio da tarefa a planilha será bloqueada para modificações.\n",
    "- IMPORTANTE: O seu código entregue na tarefa 2 será reexecutado com estas 3 queries (assim como você fez) para garantir que os resultados batem com os seus. Os grupos que mudarem suas buscas ou pré-processamentos na tarefa 3 terão sua nota penalizada e serão desclassificados da competição.\n",
    "\n",
    "## Competição\n",
    "A competição será realizada através da comparação dos resultados das buscas criativas em uma base de documentos (corpus) utilizada pela organização do Lab.\n",
    "Para que você atinja um bom desempenho na competição, é importante se atentar ao seu processo de criação de queries e na qualidade da rotulação dos documentos, pois elas serão seus guias do quão boa sua busca está se saindo!\n",
    "\n",
    "Como métrica de avaliação, esta competição irá utilizar a média do NDCG calculado para cada query. \n",
    "\n",
    "## Ferramental\n",
    "Para executar este lab não é necessário uma máquina com GPU, mas a máquina deve ter capacidade de virtualização, além de possuir o Docker e o Python instalado.\n",
    "\n",
    "Será utilizado o ElasticSearch (e opcionalmente o Kibana) no ambiente docker.\n",
    "\n",
    "Para realizar o lab é recomendado o uso de Python 3.12, além de um venv ou ambiente conda.\n",
    "\n",
    "Você deve instalar as dependências do requirements.txt.\n",
    "\n",
    "## Detalhes\n",
    "Ao executar o docker compose, além de ser levantado o ElasticSearch, também é levantada uma interface visual chamada Kibana.\n",
    "\n",
    "O Kibana é um frontend para facilitar o acionamento de algumas operações do ElasticSearch e controle de configurações específicas, observar métricas etc. Ela pode ser utilizada para fins de debug. Para quem tiver curiosidade, acesse o URI: http://localhost:5601 após levantar o serviço com o docker compose.\n",
    "\n",
    "## Dúvidas\n",
    "Caso tenham dúvidas, é só entrar em contato pelo nosso servidor do Discord."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "# Código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Passos de preparação do ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/mayarabrito/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mayarabrito/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import warnings\n",
    "import subprocess\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import OrderedDict\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import sklearn\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import spacy\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 0: Subir Stack do Elastic (ElasticSearch e Kibana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pt-core-news-sm\n",
      "Successfully installed pt-core-news-sm-3.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download pt_core_news_sm # Caso o comando não funcione, execute-o no terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=\"2025-03-21T15:31:49-03:00\" level=warning msg=\"/home/mayarabrito/Documents/LabElasticSearch/docker-compose.yaml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion\"\n",
      " Container labelasticsearch-setup-1  Created\n",
      " Container labelasticsearch-es01-1  Running\n",
      " Container labelasticsearch-kibana-1  Running\n",
      " Container labelasticsearch-setup-1  Starting\n",
      " Container labelasticsearch-setup-1  Started\n",
      " Container labelasticsearch-setup-1  Waiting\n",
      " Container labelasticsearch-setup-1  Healthy\n",
      " Container labelasticsearch-es01-1  Waiting\n",
      " Container labelasticsearch-es01-1  Healthy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.call([\"docker\", \"compose\", \"up\", \"-d\"])\n",
    "# Se esse comando falhar ou retornar 1, execute-o diretamente no terminal para identificar o erro.\n",
    "# PS: O docker deve estar instalado e rodando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 1: Baixar dados do Lupa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base de dados de notícias da Lupa\n",
    "url = \"https://docs.google.com/uc?export=download&confirm=t&id=1W067Md2EbvVzW1ufzFg17Hf7Y9cCZxxr\"\n",
    "filename = \"articles_lupa_lab_elasticsearch.zip\"\n",
    "data_path = \"data\"\n",
    "zip_file_path = f\"{data_path}/{filename}\"\n",
    "\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "# Baixa o zip\n",
    "with open(zip_file_path, \"wb\") as f:\n",
    "    f.write(requests.get(url, allow_redirects=True).content)\n",
    "\n",
    "# Extrai o csv do zip\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_path)\n",
    "    \n",
    "output_file = f\"{data_path}/articles_lupa.csv\"\n",
    "assert os.path.exists(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 2: Pré-processar os dados e gerar embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementações de pré-processamentos de texto. Modifiquem, adicionem, removam conforme necessário.\n",
    "class Preprocessors:\n",
    "    STOPWORDS = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.spacy_nlp = spacy.load(\"pt_core_news_sm\") # Utiliza para lematização\n",
    "        \n",
    "    # Remove stopwords do português\n",
    "    def remove_stopwords(self, text):\n",
    "        # Tokeniza as palavras\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove as stop words\n",
    "        tokens = [word for word in tokens if word not in self.STOPWORDS]\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    # Realiza a lematização\n",
    "    def lemma(self, text):\n",
    "        return \" \".join([token.lemma_ for token in self.spacy_nlp(text)])\n",
    "    \n",
    "    # Realiza a stemização\n",
    "    def porter_stemmer(self, text):\n",
    "        # Tokeniza as palavras\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        for index in range(len(tokens)):\n",
    "            # Realiza a stemização\n",
    "            stem_word = self.stemmer.stem(tokens[index])\n",
    "            tokens[index] = stem_word\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    # Transforma o texto em lower case\n",
    "    def lower_case(self, str):\n",
    "        return str.lower()\n",
    "\n",
    "    # Remove urls com regex\n",
    "    def remove_urls(self, text):\n",
    "        url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "        without_urls = re.sub(pattern=url_pattern, repl=' ', string=text)\n",
    "        return without_urls\n",
    "\n",
    "    # Remove números com regex\n",
    "    def remove_numbers(self, text):\n",
    "        number_pattern = r'\\d+'\n",
    "        without_number = re.sub(pattern=number_pattern,\n",
    "    repl=\" \", string=text)\n",
    "        return without_number\n",
    "\n",
    "    # Converte caracteres acentuados para sua versão ASCII\n",
    "    def accented_to_ascii(self, text):\n",
    "        text = unidecode.unidecode(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geração de embeddings finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Carregar o modelo gerador de embeddings\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Caminho para salvar o dataframe de notícias\n",
    "data_df_path = \"data/data_df.pkl\"\n",
    "\n",
    "# Selecione diferentes pré-processamentos\n",
    "# Exemplo:\n",
    "\"\"\"\n",
    "preprocessor = Preprocessors()\n",
    "preprocessing_steps = [\n",
    "    preprocessor.remove_urls,\n",
    "    preprocessor.remove_stopwords,\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "preprocessing_steps = [\n",
    "    # Adicione os pré-processamentos aqui\n",
    "]\n",
    "\n",
    "RECREATE_DF = True\n",
    "\n",
    "# Cria o data frame se ele já existir ou se a variável RECREATE_INDEX for verdadeira\n",
    "# Ou (exclusivo) carrega o dataframe salvo\n",
    "if not os.path.exists(data_df_path) or RECREATE_DF:    \n",
    "    df = pd.read_csv(output_file, sep=\";\")[[\"Título\", \"Texto\", \"Data de Publicação\"]]\n",
    "    df[\"Data de Publicação\"] = df[\"Data de Publicação\"].apply(lambda str_date: datetime.strptime(str_date.split(\" - \")[0], \"%d.%m.%Y\"))\n",
    "    df.sort_values(\"Data de Publicação\", inplace=True, ascending=False)\n",
    "    df[\"Embeddings\"] = [None] * len(df)\n",
    "    df[\"doc_id\"] = df.reset_index(drop=True).index\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        texto_completo = row[\"Texto\"].strip() + \"\\n\" + row[\"Título\"].strip()\n",
    "        \n",
    "        df.at[i, \"Texto completo\"] = texto_completo\n",
    "        texto_processado = texto_completo\n",
    "        for preprocessing_step in preprocessing_steps:\n",
    "            texto_processado = preprocessing_step(texto_processado)\n",
    "        \n",
    "        df.at[i, \"Texto processado\"] = texto_processado\n",
    "        embeddings = model.encode(texto_completo).tolist()\n",
    "        df.at[i, \"Embeddings\"] = embeddings\n",
    "        \n",
    "    print(\"Geração de embeddings finalizada.\")\n",
    "    \n",
    "    with open(data_df_path, \"wb\") as f:\n",
    "        df.to_pickle(f)\n",
    "else:\n",
    "    with open(data_df_path, \"rb\") as f:\n",
    "        df = pd.read_pickle(f)\n",
    "    print(\"Dataframe carregado de arquivo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 3: Indexar dados no ElasticSearch (Lembrem-se de reindexar os dados se os pré-processamentos mudarem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "    hosts = [{'host': \"localhost\", 'port': 9200, \"scheme\": \"https\"}],\n",
    "    basic_auth=(\"elastic\",\"elastic\"),\n",
    "    verify_certs = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice 'verificacoes_lupa' criado.\n",
      "Índice preenchido.\n",
      "Indexação finalizada.\n"
     ]
    }
   ],
   "source": [
    "RECREATE_INDEX = True\n",
    "\n",
    "index_name = \"verificacoes_lupa\"\n",
    "\n",
    "# Se a flag for True e se o índice existir, ele é deletado\n",
    "if RECREATE_INDEX and es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "    print(f\"Índice '{index_name}' deletado.\")\n",
    "\n",
    "# Cria o índice e popula com os dados\n",
    "if not es.indices.exists(index=index_name):\n",
    "    es.indices.create(index=index_name, mappings={\n",
    "        \"properties\": {\n",
    "            \"doc_id\": {\"type\": \"integer\"},\n",
    "            \"full_text\": {\"type\": \"text\"},\n",
    "            \"processed_text\": {\"type\": \"text\"},\n",
    "            \"embeddings\": {\"type\": \"dense_vector\", \"dims\": 384}\n",
    "        }\n",
    "    })\n",
    "    print(f\"Índice '{index_name}' criado.\")\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        es.index(index=index_name, id=row[\"doc_id\"], body={\n",
    "            \"doc_id\": row[\"doc_id\"],\n",
    "            \"full_text\": row[\"Texto completo\"],\n",
    "            \"processed_text\": row[\"Texto processado\"],\n",
    "            \"embeddings\": row[\"Embeddings\"]\n",
    "        })\n",
    "    print(\"Índice preenchido.\")\n",
    "\n",
    "print(\"Indexação finalizada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarefa 1: Criar query\n",
    "Crie as queries da tarefa 1 (QP1 e QP2), as queries devem ser perguntas ou declarações de até 100 caracteres.\n",
    "\n",
    "Inspire-se nas notícias presente no csv (data/articles_lupa.csv) para gerar queries interessantes.\n",
    "\n",
    "Submetam as queries ao formulário."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarefa 2: Implementação e realização das buscas\n",
    "Agora que você montou suas queries, realize cada uma das buscas para cada uma das queries (QF1; QF2; QP1 e QP2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estas serão as queries QF1 e QF2\n",
    "with open(\"data/queries_fixadas.txt\", \"r\") as f:\n",
    "    queries_fixadas = [line.strip() for line in f.readlines()]\n",
    "    assert len(queries_fixadas) == 2\n",
    "    QF1 = queries_fixadas[0]\n",
    "    QF2 = queries_fixadas[1]\n",
    "    \n",
    "# Preencha aqui as queries do grupo\n",
    "QP1 = \"Como se proteger de novos golpes online\"\n",
    "QP2 = \"Existem leis anti discriminação e violência política no Brasil?\"\n",
    "\n",
    "queries = OrderedDict()\n",
    "# queries[\"QF1\"] = QF1\n",
    "# queries[\"QF2\"] = QF2\n",
    "# queries[\"QP1\"] = QP1\n",
    "queries[\"QP2\"] = QP2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Busca Léxica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação de busca esparsa (léxica) com BM25\n",
    "def lexical_search(queries: dict[str, str]):\n",
    "    lexical_results = {}\n",
    "    for query_id, query in queries.items():\n",
    "        \n",
    "        # Pré-processa os dados\n",
    "        for preprocessing_step in preprocessing_steps:\n",
    "            query = preprocessing_step(query)\n",
    "        \n",
    "        search_query = {\n",
    "            \"size\": 100,\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"processed_text\": query\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Realiza a busca\n",
    "        response = es.search(index=index_name, body=search_query)\n",
    "        \n",
    "        hits_results = []\n",
    "        # Recupera os resultados\n",
    "        for hit in response[\"hits\"][\"hits\"]:\n",
    "            hits_results.append((hit[\"_source\"][\"doc_id\"], hit[\"_score\"]))\n",
    "        lexical_results[query_id] = hits_results\n",
    "        \n",
    "    return lexical_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Busca Semântica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza busca semântica (densa) com KNN exato\n",
    "def semantic_search(queries: dict[str, str]):\n",
    "    semantic_results = {}\n",
    "    \n",
    "    for query in queries:\n",
    "        # Aplica todos os pré-processamentos aos dados\n",
    "        for preprocessing_step in preprocessing_steps:\n",
    "            query = preprocessing_step(query)\n",
    "            \n",
    "        query_vector = model.encode(query).tolist()\n",
    "        \n",
    "        \n",
    "        search_query = {\n",
    "            \"size\": 100,\n",
    "            \"query\": {\n",
    "                \"script_score\": {\n",
    "                    \"query\": {\"match_all\": {}},\n",
    "                    \"script\": {\n",
    "                        \"source\": \"cosineSimilarity(params.query_vector, 'embeddings') + 1.0\",\n",
    "                        \"params\": {\"query_vector\": query_vector}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Realiza a busca\n",
    "        response = es.search(index=index_name, body=search_query)\n",
    "        \n",
    "        hits_results = []\n",
    "        # Recupera top 10 resultados\n",
    "        for hit in response[\"hits\"][\"hits\"]:\n",
    "            hits_results.append((hit[\"_source\"][\"doc_id\"], hit[\"_score\"]))\n",
    "            \n",
    "        semantic_results[query] = hits_results\n",
    "\n",
    "    return semantic_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Busca Híbrida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca híbrida ou RRF. Implemente sua solução aqui. Você pode realizar as duas buscas anteriores (léxica e semântica) como base para formar a busca híbrida.\n",
    "def hybrid_search(queries: dict[str, str]):\n",
    "    ## TODO: Implementar busca híbrida\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Busca Criativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemente sua própria estratégia de busca, podendo ela ser esparsa, densa ou híbrida. Implemente algo como \"more_like_this\", \"BM35\", \"fuzzy\" etc.\n",
    "def creative_search(queries: dict[str, str]):\n",
    "    ## TODO: Implementar busca híbrida\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execução das buscas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_functions = [\n",
    "    (\"lexical\", lexical_search),\n",
    "    (\"semantic\", semantic_search),\n",
    "    # (\"hybrid\", hybrid_search),\n",
    "    # (\"creative\", creative_search)\n",
    "]\n",
    "\n",
    "def run_all_searches(queries: dict[str, str]):\n",
    "    all_search_results = {}\n",
    "    for search_name, search_function in search_functions:\n",
    "        results = search_function(queries)\n",
    "        all_search_results[search_name] = results\n",
    "    return all_search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analise os resultados da busca e aprimore a busca!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexical</th>\n",
       "      <th>semantic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>[(4111, 8.562815), (4151, 8.562815), (212, 8.28064), (1136, 7.836954), (4039, 7.834598), (928, 7.7684507), (1541, 7.709979), (1118, 7.6855407), (909, 7.5722284), (1801, 7.5154157), (1853, 7.5069494), (1750, 7.49358), (1684, 7.223804), (874, 7.1097927), (1012, 7.1020064), (2876, 6.929549), (553, 6.8119273), (4499, 6.702853), (1737, 6.617267), (186, 6.416929), (4106, 6.369), (2883, 6.348332), (427, 6.3291497), (1893, 6.318362), (1137, 6.08158), (4172, 5.924452), (32, 5.922223), (1252, 5.7969394), (3931, 5.7845607), (2570, 5.778396), (1484, 5.7599807), (59, 5.716809), (612, 5.716809), (1532, 5.6473145), (3520, 5.5630317), (3116, 5.5437546), (4003, 5.539672), (2897, 5.537912), (4037, 5.523423), (3000, 5.5107284), (1264, 5.46987), (251, 5.369981), (1884, 5.369981), (2456, 5.369981), (4201, 5.369981), (4108, 5.36296), (3713, 5.2790294), (63, 5.2667613), (722, 5.251458), (1187, 5.251458), (3658, 5.1476316), (750, 5.1380544), (1394, 5.1380544), (1211, 5.1380544), (1924, 5.1380544), (3738, 5.1380544), (4482, 5.1206045), (2191, 5.0679536), (962, 5.0294447), (3098, 4.9253316), (609, 4.903529), (2889, 4.8426814), (1079, 4.8383603), (3216, 4.7753973), (3565, 4.7753973), (4471, 4.7458315), (3180, 4.6847196), (2282, 4.667619), (2351, 4.667619), (1037, 4.654605), (4285, 4.654605), (265, 4.6373434), (3078, 4.6373434), (1945, 4.6313906), (922, 4.564599), (340, 4.564599), (1669, 4.564599), (2585, 4.564599), (2736, 4.564599), (4993, 4.564599), (3613, 4.510476), (184, 4.4660273), (4845, 4.4660273), (333, 4.4633594), (3744, 4.432506), (4451, 4.432506), (3844, 4.4020786), (1655, 4.3838224), (260, 4.371623), (1685, 4.371623), (2906, 4.371623), (4360, 4.371623), (3755, 4.3019576), (1866, 4.2945685), (2850, 4.281128), (3639, 4.281128), (4369, 4.281128), (4584, 4.281128), (1930, 4.267461), (1423, 4.2656546)]</td>\n",
       "      <td>[(5041, 1.3336502), (4924, 1.3222085), (5027, 1.3161311), (3486, 1.3141042), (4698, 1.312715), (5128, 1.3116834), (5006, 1.304501), (4314, 1.2933444), (5104, 1.2929908), (4380, 1.2918205), (2592, 1.291152), (1897, 1.2880412), (3949, 1.2862076), (4835, 1.2860878), (3825, 1.2855299), (4456, 1.280465), (4639, 1.2801411), (1444, 1.2756518), (3405, 1.274065), (4624, 1.2719187), (5014, 1.2682339), (3581, 1.2649338), (3822, 1.2647045), (4527, 1.2646922), (2717, 1.2646068), (3300, 1.2640986), (3244, 1.2622895), (4988, 1.2605004), (2035, 1.2582812), (4660, 1.2562395), (308, 1.2558099), (4690, 1.253907), (4990, 1.2535738), (5080, 1.2526674), (4726, 1.2517444), (1052, 1.2516102), (4867, 1.25059), (4813, 1.250579), (4798, 1.2500643), (1298, 1.2495296), (3416, 1.249456), (4452, 1.2491118), (4815, 1.2461203), (4482, 1.2458875), (1082, 1.2452447), (1234, 1.2450281), (4923, 1.244817), (4760, 1.2441745), (2347, 1.2439706), (5029, 1.2432432), (3662, 1.2432407), (4872, 1.2430694), (4617, 1.2430043), (4968, 1.2429607), (1782, 1.2429525), (4268, 1.2426822), (3718, 1.2424458), (5012, 1.242342), (3166, 1.2421731), (3481, 1.2416722), (1589, 1.2413651), (1794, 1.2412785), (3247, 1.2411244), (5084, 1.2407681), (3049, 1.2403921), (1078, 1.2401069), (3332, 1.2400185), (2662, 1.2394726), (2190, 1.2381052), (3210, 1.2379578), (1954, 1.237947), (3025, 1.2373688), (3411, 1.2371005), (3064, 1.2368948), (5083, 1.2368925), (2176, 1.23609), (3856, 1.2354978), (4748, 1.2350229), (5119, 1.2349387), (3811, 1.2347972), (3877, 1.2342296), (4943, 1.2339023), (4620, 1.2333158), (3550, 1.2327958), (4960, 1.2324013), (4367, 1.232302), (5069, 1.2313588), (1357, 1.2313232), (3667, 1.2312287), (3938, 1.231133), (1332, 1.231102), (1423, 1.2309265), (5123, 1.2300016), (3749, 1.2299484), (4768, 1.2298275), (1510, 1.2291154), (4794, 1.2284276), (2913, 1.2270494), (311, 1.2267779), (4916, 1.2263016)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  lexical  \\\n",
       "QP2  [(4111, 8.562815), (4151, 8.562815), (212, 8.28064), (1136, 7.836954), (4039, 7.834598), (928, 7.7684507), (1541, 7.709979), (1118, 7.6855407), (909, 7.5722284), (1801, 7.5154157), (1853, 7.5069494), (1750, 7.49358), (1684, 7.223804), (874, 7.1097927), (1012, 7.1020064), (2876, 6.929549), (553, 6.8119273), (4499, 6.702853), (1737, 6.617267), (186, 6.416929), (4106, 6.369), (2883, 6.348332), (427, 6.3291497), (1893, 6.318362), (1137, 6.08158), (4172, 5.924452), (32, 5.922223), (1252, 5.7969394), (3931, 5.7845607), (2570, 5.778396), (1484, 5.7599807), (59, 5.716809), (612, 5.716809), (1532, 5.6473145), (3520, 5.5630317), (3116, 5.5437546), (4003, 5.539672), (2897, 5.537912), (4037, 5.523423), (3000, 5.5107284), (1264, 5.46987), (251, 5.369981), (1884, 5.369981), (2456, 5.369981), (4201, 5.369981), (4108, 5.36296), (3713, 5.2790294), (63, 5.2667613), (722, 5.251458), (1187, 5.251458), (3658, 5.1476316), (750, 5.1380544), (1394, 5.1380544), (1211, 5.1380544), (1924, 5.1380544), (3738, 5.1380544), (4482, 5.1206045), (2191, 5.0679536), (962, 5.0294447), (3098, 4.9253316), (609, 4.903529), (2889, 4.8426814), (1079, 4.8383603), (3216, 4.7753973), (3565, 4.7753973), (4471, 4.7458315), (3180, 4.6847196), (2282, 4.667619), (2351, 4.667619), (1037, 4.654605), (4285, 4.654605), (265, 4.6373434), (3078, 4.6373434), (1945, 4.6313906), (922, 4.564599), (340, 4.564599), (1669, 4.564599), (2585, 4.564599), (2736, 4.564599), (4993, 4.564599), (3613, 4.510476), (184, 4.4660273), (4845, 4.4660273), (333, 4.4633594), (3744, 4.432506), (4451, 4.432506), (3844, 4.4020786), (1655, 4.3838224), (260, 4.371623), (1685, 4.371623), (2906, 4.371623), (4360, 4.371623), (3755, 4.3019576), (1866, 4.2945685), (2850, 4.281128), (3639, 4.281128), (4369, 4.281128), (4584, 4.281128), (1930, 4.267461), (1423, 4.2656546)]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     semantic  \n",
       "QP2  [(5041, 1.3336502), (4924, 1.3222085), (5027, 1.3161311), (3486, 1.3141042), (4698, 1.312715), (5128, 1.3116834), (5006, 1.304501), (4314, 1.2933444), (5104, 1.2929908), (4380, 1.2918205), (2592, 1.291152), (1897, 1.2880412), (3949, 1.2862076), (4835, 1.2860878), (3825, 1.2855299), (4456, 1.280465), (4639, 1.2801411), (1444, 1.2756518), (3405, 1.274065), (4624, 1.2719187), (5014, 1.2682339), (3581, 1.2649338), (3822, 1.2647045), (4527, 1.2646922), (2717, 1.2646068), (3300, 1.2640986), (3244, 1.2622895), (4988, 1.2605004), (2035, 1.2582812), (4660, 1.2562395), (308, 1.2558099), (4690, 1.253907), (4990, 1.2535738), (5080, 1.2526674), (4726, 1.2517444), (1052, 1.2516102), (4867, 1.25059), (4813, 1.250579), (4798, 1.2500643), (1298, 1.2495296), (3416, 1.249456), (4452, 1.2491118), (4815, 1.2461203), (4482, 1.2458875), (1082, 1.2452447), (1234, 1.2450281), (4923, 1.244817), (4760, 1.2441745), (2347, 1.2439706), (5029, 1.2432432), (3662, 1.2432407), (4872, 1.2430694), (4617, 1.2430043), (4968, 1.2429607), (1782, 1.2429525), (4268, 1.2426822), (3718, 1.2424458), (5012, 1.242342), (3166, 1.2421731), (3481, 1.2416722), (1589, 1.2413651), (1794, 1.2412785), (3247, 1.2411244), (5084, 1.2407681), (3049, 1.2403921), (1078, 1.2401069), (3332, 1.2400185), (2662, 1.2394726), (2190, 1.2381052), (3210, 1.2379578), (1954, 1.237947), (3025, 1.2373688), (3411, 1.2371005), (3064, 1.2368948), (5083, 1.2368925), (2176, 1.23609), (3856, 1.2354978), (4748, 1.2350229), (5119, 1.2349387), (3811, 1.2347972), (3877, 1.2342296), (4943, 1.2339023), (4620, 1.2333158), (3550, 1.2327958), (4960, 1.2324013), (4367, 1.232302), (5069, 1.2313588), (1357, 1.2313232), (3667, 1.2312287), (3938, 1.231133), (1332, 1.231102), (1423, 1.2309265), (5123, 1.2300016), (3749, 1.2299484), (4768, 1.2298275), (1510, 1.2291154), (4794, 1.2284276), (2913, 1.2270494), (311, 1.2267779), (4916, 1.2263016)]  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_search_results = run_all_searches(queries)\n",
    "search_results_df = pd.DataFrame(all_search_results)\n",
    "search_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados das buscas salvos em 'data/search_results.csv'.\n",
      "Documentos de interesse salvos em 'data/documents.csv'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexical</th>\n",
       "      <th>semantic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>4111</td>\n",
       "      <td>5041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>4151</td>\n",
       "      <td>4924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>212</td>\n",
       "      <td>5027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1136</td>\n",
       "      <td>3486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>4039</td>\n",
       "      <td>4698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>928</td>\n",
       "      <td>5128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1541</td>\n",
       "      <td>5006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1118</td>\n",
       "      <td>4314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>909</td>\n",
       "      <td>5104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1801</td>\n",
       "      <td>4380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1853</td>\n",
       "      <td>2592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1750</td>\n",
       "      <td>1897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1684</td>\n",
       "      <td>3949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>874</td>\n",
       "      <td>4835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1012</td>\n",
       "      <td>3825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>2876</td>\n",
       "      <td>4456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>553</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>4499</td>\n",
       "      <td>1444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1737</td>\n",
       "      <td>3405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>186</td>\n",
       "      <td>4624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>4106</td>\n",
       "      <td>5014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>2883</td>\n",
       "      <td>3581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>427</td>\n",
       "      <td>3822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1893</td>\n",
       "      <td>4527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1137</td>\n",
       "      <td>2717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>4172</td>\n",
       "      <td>3300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>32</td>\n",
       "      <td>3244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1252</td>\n",
       "      <td>4988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>3931</td>\n",
       "      <td>2035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>2570</td>\n",
       "      <td>4660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1484</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>59</td>\n",
       "      <td>4690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>612</td>\n",
       "      <td>4990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1532</td>\n",
       "      <td>5080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>3520</td>\n",
       "      <td>4726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>3116</td>\n",
       "      <td>1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>4003</td>\n",
       "      <td>4867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>2897</td>\n",
       "      <td>4813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>4037</td>\n",
       "      <td>4798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>3000</td>\n",
       "      <td>1298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1264</td>\n",
       "      <td>3416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>251</td>\n",
       "      <td>4452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1884</td>\n",
       "      <td>4815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>2456</td>\n",
       "      <td>4482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>4201</td>\n",
       "      <td>1082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>4108</td>\n",
       "      <td>1234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>3713</td>\n",
       "      <td>4923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>63</td>\n",
       "      <td>4760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>722</td>\n",
       "      <td>2347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1187</td>\n",
       "      <td>5029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>3658</td>\n",
       "      <td>3662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>750</td>\n",
       "      <td>4872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1394</td>\n",
       "      <td>4617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1211</td>\n",
       "      <td>4968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1924</td>\n",
       "      <td>1782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>3738</td>\n",
       "      <td>4268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>4482</td>\n",
       "      <td>3718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>2191</td>\n",
       "      <td>5012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>962</td>\n",
       "      <td>3166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>3098</td>\n",
       "      <td>3481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>609</td>\n",
       "      <td>1589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>2889</td>\n",
       "      <td>1794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1079</td>\n",
       "      <td>3247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>3216</td>\n",
       "      <td>5084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>3565</td>\n",
       "      <td>3049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>4471</td>\n",
       "      <td>1078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>3180</td>\n",
       "      <td>3332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>2282</td>\n",
       "      <td>2662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>2351</td>\n",
       "      <td>2190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1037</td>\n",
       "      <td>3210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>4285</td>\n",
       "      <td>1954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>265</td>\n",
       "      <td>3025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>3078</td>\n",
       "      <td>3411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1945</td>\n",
       "      <td>3064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>922</td>\n",
       "      <td>5083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>340</td>\n",
       "      <td>2176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1669</td>\n",
       "      <td>3856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>2585</td>\n",
       "      <td>4748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>2736</td>\n",
       "      <td>5119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>4993</td>\n",
       "      <td>3811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>3613</td>\n",
       "      <td>3877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>184</td>\n",
       "      <td>4943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>4845</td>\n",
       "      <td>4620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>333</td>\n",
       "      <td>3550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>3744</td>\n",
       "      <td>4960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>4451</td>\n",
       "      <td>4367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>3844</td>\n",
       "      <td>5069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1655</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>260</td>\n",
       "      <td>3667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1685</td>\n",
       "      <td>3938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>2906</td>\n",
       "      <td>1332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>4360</td>\n",
       "      <td>1423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>3755</td>\n",
       "      <td>5123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1866</td>\n",
       "      <td>3749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>2850</td>\n",
       "      <td>4768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>3639</td>\n",
       "      <td>1510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>4369</td>\n",
       "      <td>4794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>4584</td>\n",
       "      <td>2913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1930</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1423</td>\n",
       "      <td>4916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lexical  semantic\n",
       "QP2     4111      5041\n",
       "QP2     4151      4924\n",
       "QP2      212      5027\n",
       "QP2     1136      3486\n",
       "QP2     4039      4698\n",
       "QP2      928      5128\n",
       "QP2     1541      5006\n",
       "QP2     1118      4314\n",
       "QP2      909      5104\n",
       "QP2     1801      4380\n",
       "QP2     1853      2592\n",
       "QP2     1750      1897\n",
       "QP2     1684      3949\n",
       "QP2      874      4835\n",
       "QP2     1012      3825\n",
       "QP2     2876      4456\n",
       "QP2      553      4639\n",
       "QP2     4499      1444\n",
       "QP2     1737      3405\n",
       "QP2      186      4624\n",
       "QP2     4106      5014\n",
       "QP2     2883      3581\n",
       "QP2      427      3822\n",
       "QP2     1893      4527\n",
       "QP2     1137      2717\n",
       "QP2     4172      3300\n",
       "QP2       32      3244\n",
       "QP2     1252      4988\n",
       "QP2     3931      2035\n",
       "QP2     2570      4660\n",
       "QP2     1484       308\n",
       "QP2       59      4690\n",
       "QP2      612      4990\n",
       "QP2     1532      5080\n",
       "QP2     3520      4726\n",
       "QP2     3116      1052\n",
       "QP2     4003      4867\n",
       "QP2     2897      4813\n",
       "QP2     4037      4798\n",
       "QP2     3000      1298\n",
       "QP2     1264      3416\n",
       "QP2      251      4452\n",
       "QP2     1884      4815\n",
       "QP2     2456      4482\n",
       "QP2     4201      1082\n",
       "QP2     4108      1234\n",
       "QP2     3713      4923\n",
       "QP2       63      4760\n",
       "QP2      722      2347\n",
       "QP2     1187      5029\n",
       "QP2     3658      3662\n",
       "QP2      750      4872\n",
       "QP2     1394      4617\n",
       "QP2     1211      4968\n",
       "QP2     1924      1782\n",
       "QP2     3738      4268\n",
       "QP2     4482      3718\n",
       "QP2     2191      5012\n",
       "QP2      962      3166\n",
       "QP2     3098      3481\n",
       "QP2      609      1589\n",
       "QP2     2889      1794\n",
       "QP2     1079      3247\n",
       "QP2     3216      5084\n",
       "QP2     3565      3049\n",
       "QP2     4471      1078\n",
       "QP2     3180      3332\n",
       "QP2     2282      2662\n",
       "QP2     2351      2190\n",
       "QP2     1037      3210\n",
       "QP2     4285      1954\n",
       "QP2      265      3025\n",
       "QP2     3078      3411\n",
       "QP2     1945      3064\n",
       "QP2      922      5083\n",
       "QP2      340      2176\n",
       "QP2     1669      3856\n",
       "QP2     2585      4748\n",
       "QP2     2736      5119\n",
       "QP2     4993      3811\n",
       "QP2     3613      3877\n",
       "QP2      184      4943\n",
       "QP2     4845      4620\n",
       "QP2      333      3550\n",
       "QP2     3744      4960\n",
       "QP2     4451      4367\n",
       "QP2     3844      5069\n",
       "QP2     1655      1357\n",
       "QP2      260      3667\n",
       "QP2     1685      3938\n",
       "QP2     2906      1332\n",
       "QP2     4360      1423\n",
       "QP2     3755      5123\n",
       "QP2     1866      3749\n",
       "QP2     2850      4768\n",
       "QP2     3639      1510\n",
       "QP2     4369      4794\n",
       "QP2     4584      2913\n",
       "QP2     1930       311\n",
       "QP2     1423      4916"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_exploded_df(search_results_df):\n",
    "    exploded_search_results_df = pd.concat([search_results_df[col].explode() for col in search_results_df.columns], axis=1)\n",
    "    exploded_search_results_df = exploded_search_results_df.apply(lambda l: [doc_id for doc_id, _ in l])\n",
    "    return exploded_search_results_df\n",
    "\n",
    "def generate_found_docs_text_df(exploded_search_results_df, all_docs_df):\n",
    "    # Recupera os ids únicos dos documentos\n",
    "    documents_ids = set(exploded_search_results_df.to_numpy().flatten().tolist())\n",
    "\n",
    "    # Salva os textos e os ids dos documetnos que foram encontrados ems usas buscas\n",
    "    documents_df = all_docs_df[all_docs_df[\"doc_id\"].isin(documents_ids)][[\"Texto processado\", \"doc_id\"]]\n",
    "    return documents_df\n",
    "\n",
    "exploded_df = generate_exploded_df(search_results_df)\n",
    "found_docs_text_df = generate_found_docs_text_df(exploded_df, all_docs_df=df)\n",
    "\n",
    "def save_results_to_file(exploded_df: pd.DataFrame,\n",
    "                         found_docs_text_df: pd.DataFrame,\n",
    "                         exploded_df_save_filepath: str = \"data/search_results.csv\",\n",
    "                         found_docs_text_save_filepath: str = \"data/documents.csv\"):\n",
    "    exploded_df.to_csv(exploded_df_save_filepath)\n",
    "    found_docs_text_df.to_csv(found_docs_text_save_filepath)\n",
    "    print(\"Resultados das buscas salvos em 'data/search_results.csv'.\")\n",
    "    print(\"Documentos de interesse salvos em 'data/documents.csv'.\")\n",
    "    \n",
    "save_results_to_file(exploded_df, found_docs_text_df)\n",
    "exploded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anote as relevâncias na sua planilha!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarefa 3: Reexecutar a busca para as novas queries e rotular os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preencha aqui com as queries adicionais do seu grupo\n",
    "QA1 = XXXXXXXXX\n",
    "QA2 = XXXXXXXXX\n",
    "QA3 = XXXXXXXXX\n",
    "\n",
    "queries = OrderedDict()\n",
    "queries[\"QF1\"] = QF1\n",
    "queries[\"QF2\"] = QF2\n",
    "queries[\"QA1\"] = QA1\n",
    "queries[\"QA2\"] = QA2\n",
    "queries[\"QA3\"] = QA3\n",
    "\n",
    "\n",
    "all_search_results = run_all_searches(queries)\n",
    "search_results_df = pd.DataFrame(all_search_results)\n",
    "search_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_df = generate_exploded_df(search_results_df)\n",
    "found_docs_text_df = generate_found_docs_text_df(exploded_df, all_docs_df=df)\n",
    "\n",
    "def save_results_to_file(exploded_df: pd.DataFrame,\n",
    "                         found_docs_text_df: pd.DataFrame,\n",
    "                         exploded_df_save_filepath: str = \"data/search_results.csv\",\n",
    "                         found_docs_text_save_filepath: str = \"data/documents.csv\"):\n",
    "    exploded_df.to_csv(exploded_df_save_filepath)\n",
    "    found_docs_text_df.to_csv(found_docs_text_save_filepath)\n",
    "    print(\"Resultados das buscas salvos em 'data/search_results.csv'.\")\n",
    "    print(\"Documentos de interesse salvos em 'data/documents.csv'.\")\n",
    "    \n",
    "save_results_to_file(exploded_df, found_docs_text_df)\n",
    "exploded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anote os resultados na sua planilha!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A competição utilizará o NDCG médio por query para computar seu desempenho."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
